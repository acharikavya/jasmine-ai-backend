{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "toolbox.py (FINAL with robust Grad-CAM)\n",
    "\n",
    "Usage (examples):\n",
    "    python toolbox.py audit\n",
    "    python toolbox.py extract_embeddings --images_dir sample_images --out embeddings.csv\n",
    "    python toolbox.py runserver --host 127.0.0.1 --port 8000\n",
    "    python toolbox.py test_request --image sample_images/example.jpg --host 127.0.0.1 --port 8000\n",
    "\n",
    "Place your models in folder:\n",
    "    models/Cnn_model.h5\n",
    "    models/xgboost_soil_health_model.pkl\n",
    "\n",
    "Notes:\n",
    "- This version expects XGBoost trained on soil-only features listed in SOIL_FEATURES.\n",
    "- Soil form keys expected by the API are cleaned (spaces -> '_', '%' -> '_pct', '/' -> '_'):\n",
    "  PH, EC_ds_m, OC_pct, N_kg_hectre, P_kg_hectre, K_kg_hectre, S_ppm, Zn_ppm, B_ppm, Fe_ppm, Mn_ppm, Cu_ppm\n",
    "\"\"\"\n",
    "\n",
    "import sys, os, io, base64, argparse, json\n",
    "from pathlib import Path\n",
    "\n",
    "# ------- helper to ensure packages -------\n",
    "def ensure_packages(packages):\n",
    "    import importlib, subprocess, sys\n",
    "    missing = []\n",
    "    for pkg in packages:\n",
    "        try:\n",
    "            importlib.import_module(pkg)\n",
    "        except Exception:\n",
    "            missing.append(pkg)\n",
    "    if not missing:\n",
    "        return\n",
    "    print(\"Installing missing packages:\", missing)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "# packages used across modes\n",
    "ensure_packages([\n",
    "    \"tensorflow\",\n",
    "    \"numpy\",\n",
    "    \"Pillow\",\n",
    "    \"opencv_python\",\n",
    "    \"joblib\",\n",
    "    \"fastapi\",\n",
    "    \"uvicorn\",\n",
    "    \"xgboost\",\n",
    "    \"shap\",\n",
    "    \"requests\",\n",
    "    \"pandas\",\n",
    "    \"python-multipart\"\n",
    "])\n",
    "\n",
    "# heavy imports\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from fastapi import FastAPI, File, UploadFile, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "import shap\n",
    "import xgboost\n",
    "import requests\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "MODELS_DIR = Path(\"models\")\n",
    "CNN_PATH = MODELS_DIR / \"Cnn_model.h5\"                     # change if your filename differs\n",
    "XGB_PATH = MODELS_DIR / \"xgboost_soil_health_model.pkl\"    # change if your filename differs\n",
    "\n",
    "DEFAULT_IMG_SIZE = (224, 224)\n",
    "CLASS_LABELS = [\"healthy\", \"diseased\"]\n",
    "\n",
    "# Soil features (exact order from your XGBoost booster)\n",
    "SOIL_FEATURES = [\n",
    "    \"PH\", \"EC ds/m\", \"OC %\", \"N kg/hectre\", \"P kg/hectre\", \"K kg/hectre\",\n",
    "    \"S ppm\", \"Zn ppm\", \"B ppm\", \"Fe ppm\", \"Mn ppm\", \"Cu ppm\"\n",
    "]\n",
    "\n",
    "# cleaned keys mapping for multipart/form-data usage\n",
    "def clean_key(s):\n",
    "    return s.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"%\", \"_pct\")\n",
    "\n",
    "SOIL_KEYS = [clean_key(f) for f in SOIL_FEATURES]\n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "def load_cnn():\n",
    "    if not CNN_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Could not find {CNN_PATH}\")\n",
    "    print(\"Loading CNN from\", CNN_PATH)\n",
    "    model = load_model(str(CNN_PATH))\n",
    "    return model\n",
    "\n",
    "def load_xgb():\n",
    "    if not XGB_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Could not find {XGB_PATH}\")\n",
    "    print(\"Loading XGBoost from\", XGB_PATH)\n",
    "    model = joblib.load(str(XGB_PATH))\n",
    "    return model\n",
    "\n",
    "# ---------------- Audit ----------------\n",
    "def audit_models():\n",
    "    print(\"=== MODEL AUDIT ===\")\n",
    "    # CNN audit\n",
    "    try:\n",
    "        cnn = load_cnn()\n",
    "    except Exception as e:\n",
    "        print(\"Error loading cnn:\", e)\n",
    "        cnn = None\n",
    "\n",
    "    if cnn is not None:\n",
    "        print(\"\\n--- CNN Summary ---\")\n",
    "        cnn.summary()\n",
    "        inp_shape = cnn.input_shape\n",
    "        print(f\"\\nCNN input_shape: {inp_shape}\")\n",
    "        # last conv layer\n",
    "        last_conv = None\n",
    "        for layer in reversed(cnn.layers):\n",
    "            if \"conv\" in layer.name.lower():\n",
    "                last_conv = layer.name\n",
    "                break\n",
    "        print(\"Last conv layer (for Grad-CAM):\", last_conv)\n",
    "        # try to inspect global pooling\n",
    "        try:\n",
    "            gap = cnn.get_layer(\"global_average_pooling2d\")\n",
    "            print(\"GlobalAveragePooling2D layer found:\", gap.output_shape)\n",
    "        except Exception:\n",
    "            print(\"No named GlobalAveragePooling2D layer found by that name.\")\n",
    "        # penultimate layer\n",
    "        penultimate = None\n",
    "        if len(cnn.layers) >= 2:\n",
    "            penultimate = cnn.layers[-2].name\n",
    "            print(\"Penultimate layer:\", penultimate)\n",
    "            # get embedding size with dummy\n",
    "            try:\n",
    "                embed_model = Model(inputs=cnn.input, outputs=cnn.get_layer(\"global_average_pooling2d\").output)\n",
    "                H = inp_shape[1] or DEFAULT_IMG_SIZE[0]\n",
    "                W = inp_shape[2] or DEFAULT_IMG_SIZE[1]\n",
    "                C = inp_shape[3] or 3\n",
    "                dummy = np.zeros((1, H, W, C), dtype=\"float32\")\n",
    "                emb = embed_model.predict(dummy)\n",
    "                emb_dim = int(np.prod(emb.shape[1:]))\n",
    "                print(\"Embedding shape (from global_average_pooling2d):\", emb.shape, \"flattened dim:\", emb_dim)\n",
    "            except Exception as e:\n",
    "                print(\"Could not compute embedding on dummy input with global pool:\", e)\n",
    "        else:\n",
    "            print(\"CNN too shallow to determine penultimate layer.\")\n",
    "\n",
    "    # XGBoost audit\n",
    "    try:\n",
    "        xgb = load_xgb()\n",
    "    except Exception as e:\n",
    "        print(\"Error loading xgboost:\", e)\n",
    "        xgb = None\n",
    "\n",
    "    if xgb is not None:\n",
    "        print(\"\\n--- XGBoost Info ---\")\n",
    "        try:\n",
    "            booster = xgb.get_booster()\n",
    "            f_names = booster.feature_names\n",
    "            print(\"Booster feature names:\", f_names)\n",
    "            if f_names:\n",
    "                print(\"Number of features (from booster):\", len(f_names))\n",
    "        except Exception as e:\n",
    "            print(\"Could not read booster.feature_names:\", e)\n",
    "        print(\"Supports predict_proba:\", hasattr(xgb, \"predict_proba\"))\n",
    "        # try probing with guesses (we won't expect success because booster has names)\n",
    "        guesses = [64, 128, 256, 512, 768, 1024]\n",
    "        probed = None\n",
    "        for g in guesses:\n",
    "            try:\n",
    "                _ = xgb.predict(np.zeros((1, g)))\n",
    "                probed = g\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if probed:\n",
    "            print(\"XGBoost accepted input length (probing):\", probed)\n",
    "        else:\n",
    "            print(\"Could not infer XGBoost feature length by probing. Check training code or booster feature names.\")\n",
    "\n",
    "    print(\"\\n=== AUDIT COMPLETE ===\\n\")\n",
    "\n",
    "# ------------- Embeddings -------------\n",
    "def extract_embeddings(images_dir: str, out_csv: str):\n",
    "    cnn = load_cnn()\n",
    "    inp_shape = cnn.input_shape\n",
    "    H = inp_shape[1] or DEFAULT_IMG_SIZE[0]\n",
    "    W = inp_shape[2] or DEFAULT_IMG_SIZE[1]\n",
    "    # use global average pooling2d as embed model\n",
    "    try:\n",
    "        embed_model = Model(inputs=cnn.input, outputs=cnn.get_layer(\"global_average_pooling2d\").output)\n",
    "        print(\"Using global_average_pooling2d for embeddings.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error creating embed model from global pool:\", e)\n",
    "        # fallback to penultimate\n",
    "        try:\n",
    "            embed_model = Model(inputs=cnn.input, outputs=cnn.layers[-2].output)\n",
    "            print(\"Fallback: using layers[-2] for embeddings.\")\n",
    "        except Exception as e2:\n",
    "            print(\"Could not create embedding extractor model. Inspect your CNN architecture.\")\n",
    "            raise RuntimeError(\"No embedding extractor available.\")\n",
    "\n",
    "    images = list(Path(images_dir).glob(\"*\"))\n",
    "    rows = []\n",
    "    print(f\"Found {len(images)} files in {images_dir}. Extracting embeddings...\")\n",
    "    for p in images:\n",
    "        if p.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\"):\n",
    "            continue\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\").resize((W, H))\n",
    "            arr = np.array(img).astype(\"float32\") / 255.0\n",
    "            x = np.expand_dims(arr, 0)\n",
    "            emb = embed_model.predict(x)\n",
    "            emb_flat = emb.reshape(-1)\n",
    "            row = {\"image\": p.name}\n",
    "            for i, v in enumerate(emb_flat):\n",
    "                row[f\"e{i}\"] = float(v)\n",
    "            rows.append(row)\n",
    "        except Exception as e:\n",
    "            print(\"Error on\", p, e)\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(\"Saved embeddings to\", out_csv)\n",
    "\n",
    "# ------------- Grad-CAM (robust) -------------\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name=None, class_index=None, IMG_SIZE=DEFAULT_IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Robust Grad-CAM: performs forward pass inside GradientTape so gradients can be computed\n",
    "    for nested/sequential models. Returns heatmap (H,W) and predicted class index.\n",
    "    \"\"\"\n",
    "    # pick last conv layer if not provided\n",
    "    if last_conv_layer_name is None:\n",
    "        for layer in reversed(model.layers):\n",
    "            if \"conv\" in layer.name.lower():\n",
    "                last_conv_layer_name = layer.name\n",
    "                break\n",
    "    if last_conv_layer_name is None:\n",
    "        raise ValueError(\"No conv layer found and last_conv_layer_name not provided.\")\n",
    "\n",
    "    # helper to call layer in inference mode\n",
    "    def call_layer(layer, x):\n",
    "        try:\n",
    "            return layer(x, training=False)\n",
    "        except TypeError:\n",
    "            return layer(x)\n",
    "\n",
    "    x = tf.convert_to_tensor(img_array, dtype=tf.float32)\n",
    "\n",
    "    # Forward pass inside the tape so TensorFlow records ops\n",
    "    conv_outputs = None\n",
    "    out = x\n",
    "    with tf.GradientTape() as tape:\n",
    "        # we will watch conv_outputs once we create it\n",
    "        for layer in model.layers:\n",
    "            out = call_layer(layer, out)\n",
    "            if layer.name == last_conv_layer_name:\n",
    "                conv_outputs = out\n",
    "                tape.watch(conv_outputs)\n",
    "        preds = out  # final predictions after full forward pass\n",
    "        if class_index is None:\n",
    "            class_index = tf.argmax(preds[0])\n",
    "        loss = preds[:, class_index]\n",
    "\n",
    "    # Compute gradients of the loss w.r.t. the conv outputs\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    if grads is None:\n",
    "        raise RuntimeError(\"Gradients are None â€” model may not be differentiable for this output.\")\n",
    "    grads = grads[0]  # remove batch dim\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
    "\n",
    "    conv_out = conv_outputs[0]  # remove batch dim\n",
    "    cam = tf.zeros(conv_out.shape[0:2], dtype=tf.float32)\n",
    "    for i in range(conv_out.shape[-1]):\n",
    "        cam += pooled_grads[i] * conv_out[:, :, i]\n",
    "    cam = tf.maximum(cam, 0)\n",
    "    denom = tf.math.reduce_max(cam) + 1e-8\n",
    "    cam = cam / denom\n",
    "    cam = cam.numpy()\n",
    "    cam = cv2.resize(cam, (IMG_SIZE[1], IMG_SIZE[0]))\n",
    "    return cam, int(class_index)\n",
    "\n",
    "def overlay_heatmap_on_pil(img_pil, heatmap, alpha=0.4):\n",
    "    img = np.array(img_pil.resize((heatmap.shape[1], heatmap.shape[0]))).astype(\"uint8\")\n",
    "    heatmap_uint8 = np.uint8(255 * heatmap)\n",
    "    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(heatmap_color, alpha, img, 1 - alpha, 0)\n",
    "    return Image.fromarray(overlay)\n",
    "\n",
    "# ------------- FastAPI server -------------\n",
    "def create_app():\n",
    "    cnn = None\n",
    "    embed_model = None\n",
    "    xgb = None\n",
    "    shap_explainer = None\n",
    "    last_conv_layer = None\n",
    "\n",
    "    print(\"Loading models for server...\")\n",
    "    # load cnn and build embed model (global pool)\n",
    "    try:\n",
    "        cnn = load_cnn()\n",
    "        try:\n",
    "            embed_model = Model(inputs=cnn.input, outputs=cnn.get_layer(\"global_average_pooling2d\").output)\n",
    "            print(\"Embed model created from global_average_pooling2d.\")\n",
    "        except Exception:\n",
    "            try:\n",
    "                embed_model = Model(inputs=cnn.input, outputs=cnn.layers[-2].output)\n",
    "                print(\"Embed model created from layers[-2].\")\n",
    "            except Exception:\n",
    "                embed_model = None\n",
    "                print(\"No embed model available.\")\n",
    "        # last conv for gradcam\n",
    "        for layer in reversed(cnn.layers):\n",
    "            if \"conv\" in layer.name.lower():\n",
    "                last_conv_layer = layer.name\n",
    "                break\n",
    "        print(\"Last conv layer set to:\", last_conv_layer)\n",
    "    except Exception as e:\n",
    "        print(\"CNN load error:\", e)\n",
    "\n",
    "    try:\n",
    "        xgb = load_xgb()\n",
    "    except Exception as e:\n",
    "        print(\"XGB load error:\", e)\n",
    "\n",
    "    # build shap explainer if xgb present; use background zeros with correct feature count\n",
    "    if xgb is not None:\n",
    "        try:\n",
    "            feature_count = len(SOIL_FEATURES)\n",
    "            background = np.zeros((1, feature_count))\n",
    "            shap_explainer = shap.TreeExplainer(xgb, data=background)\n",
    "            print(\"SHAP explainer ready with background shape:\", background.shape)\n",
    "        except Exception as e:\n",
    "            print(\"SHAP init error:\", e)\n",
    "            shap_explainer = None\n",
    "\n",
    "    app = FastAPI()\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    async def root():\n",
    "        return {\"status\": \"ok\", \"note\": \"POST /predict with form fields image + soil fields (see docs)\"}\n",
    "\n",
    "    @app.post(\"/predict\")\n",
    "    async def predict(request: Request, image: UploadFile = File(...)):\n",
    "        \"\"\"\n",
    "        Expects multipart/form-data with:\n",
    "         - file 'image'\n",
    "         - soil fields as form fields (cleaned keys): PH, EC_ds_m, OC_pct, N_kg_hectre, P_kg_hectre, K_kg_hectre,\n",
    "           S_ppm, Zn_ppm, B_ppm, Fe_ppm, Mn_ppm, Cu_ppm\n",
    "        Returns json with label/confidence/gradcam_overlay_b64/shap_top_features/advice\n",
    "        \"\"\"\n",
    "        if cnn is None:\n",
    "            return JSONResponse({\"error\": \"CNN not loaded on server.\"}, status_code=500)\n",
    "\n",
    "        # read image bytes\n",
    "        contents = await image.read()\n",
    "        img_pil = Image.open(io.BytesIO(contents)).convert(\"RGB\")\n",
    "        inp_shape = cnn.input_shape\n",
    "        H = inp_shape[1] or DEFAULT_IMG_SIZE[0]\n",
    "        W = inp_shape[2] or DEFAULT_IMG_SIZE[1]\n",
    "        img_resized = img_pil.resize((W, H))\n",
    "        arr = np.array(img_resized).astype(\"float32\") / 255.0\n",
    "        x = np.expand_dims(arr, 0)\n",
    "\n",
    "        # cnn predict\n",
    "        preds = cnn.predict(x)[0]\n",
    "        idx = int(np.argmax(preds))\n",
    "        conf = float(np.max(preds))\n",
    "        label = CLASS_LABELS[idx] if CLASS_LABELS else str(idx)\n",
    "\n",
    "        # grad-cam\n",
    "        overlay_b64 = None\n",
    "        try:\n",
    "            heatmap, _ = make_gradcam_heatmap(x, cnn, last_conv_layer, class_index=idx, IMG_SIZE=(H, W))\n",
    "            overlay = overlay_heatmap_on_pil(img_pil, heatmap)\n",
    "            buff = io.BytesIO()\n",
    "            overlay.save(buff, format=\"PNG\")\n",
    "            overlay_b64 = base64.b64encode(buff.getvalue()).decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(\"Grad-CAM error:\", e)\n",
    "\n",
    "        # Build soil-only input for XGBoost using SOIL_KEYS order (cleaned names)\n",
    "        form = await request.form()\n",
    "        soil_vals = []\n",
    "        for key in SOIL_KEYS:\n",
    "            val = form.get(key)\n",
    "            if val is None:\n",
    "                # allow alternate key names in case frontend uses original labels\n",
    "                # try to match original feature text\n",
    "                orig_idx = SOIL_KEYS.index(key)\n",
    "                orig_name = SOIL_FEATURES[orig_idx]\n",
    "                # try raw name\n",
    "                val = form.get(orig_name)\n",
    "            try:\n",
    "                soil_vals.append(float(val) if val is not None else 0.0)\n",
    "            except Exception:\n",
    "                soil_vals.append(0.0)\n",
    "\n",
    "        model_input = np.array(soil_vals, dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "        xgb_conf = None\n",
    "        shap_top = None\n",
    "        try:\n",
    "            if xgb is not None:\n",
    "                if hasattr(xgb, \"predict_proba\"):\n",
    "                    xgb_probs = xgb.predict_proba(model_input)[0]\n",
    "                    xgb_idx = int(np.argmax(xgb_probs))\n",
    "                    xgb_conf = float(np.max(xgb_probs))\n",
    "                else:\n",
    "                    xgb_idx = int(xgb.predict(model_input)[0])\n",
    "                    xgb_conf = None\n",
    "\n",
    "                # SHAP: compute contribution vector and pick top3 features by absolute value\n",
    "                if shap_explainer is not None:\n",
    "                    try:\n",
    "                        sv = shap_explainer.shap_values(model_input)\n",
    "                        if isinstance(sv, list):\n",
    "                            arr = np.array(sv[0]).reshape(-1)\n",
    "                        else:\n",
    "                            arr = np.array(sv).reshape(-1)\n",
    "                        idxs = np.argsort(np.abs(arr))[-3:][::-1]\n",
    "                        shap_top = []\n",
    "                        for i in idxs:\n",
    "                            shap_top.append({\n",
    "                                \"feature\": SOIL_FEATURES[i],\n",
    "                                \"key\": SOIL_KEYS[i],\n",
    "                                \"shap_value\": float(arr[i])\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(\"SHAP compute error:\", e)\n",
    "                        shap_top = None\n",
    "        except Exception as e:\n",
    "            print(\"XGB/SHAP error:\", e)\n",
    "\n",
    "        # simple severity & advice heuristics\n",
    "        severity = \"Low\"\n",
    "        if conf >= 0.85 or (xgb_conf is not None and xgb_conf >= 0.85):\n",
    "            severity = \"High\" if (CLASS_LABELS and not CLASS_LABELS[0].lower().startswith(\"healthy\")) or (CLASS_LABELS is None and label != \"0\") else \"Low\"\n",
    "        elif conf >= 0.6:\n",
    "            severity = \"Medium\"\n",
    "\n",
    "        advice = \"No issue detected. Keep regular care.\" if (CLASS_LABELS is None and label == \"0\") or (CLASS_LABELS and label.lower().startswith(\"healthy\")) else \"Remove affected leaves, improve ventilation, consider treatment if it spreads.\"\n",
    "\n",
    "        return {\n",
    "            \"label\": label,\n",
    "            \"confidence\": conf,\n",
    "            \"xgb_confidence\": xgb_conf,\n",
    "            \"severity\": severity,\n",
    "            \"advice\": advice,\n",
    "            \"gradcam_overlay_b64\": overlay_b64,\n",
    "            \"shap_top_features\": shap_top\n",
    "        }\n",
    "\n",
    "    return app\n",
    "\n",
    "# ------------- Test client -------------\n",
    "def send_test_request(server_url, image_path, soil_dict):\n",
    "    files = {\"image\": open(image_path, \"rb\")}\n",
    "    # soil_dict keys should be cleaned keys matching SOIL_KEYS\n",
    "    data = soil_dict\n",
    "    print(\"Sending request to\", server_url)\n",
    "    r = requests.post(server_url + \"/predict\", files=files, data=data)\n",
    "    try:\n",
    "        print(\"Status\", r.status_code)\n",
    "        print(\"Response JSON:\")\n",
    "        print(json.dumps(r.json(), indent=2))\n",
    "    except Exception as e:\n",
    "        print(\"Error reading response:\", e)\n",
    "        print(r.text)\n",
    "\n",
    "# ---------------- CLI ----------------\n",
    "def main():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"mode\", choices=[\"audit\", \"extract_embeddings\", \"runserver\", \"test_request\"], help=\"mode to run\")\n",
    "    p.add_argument(\"--images_dir\", default=\"sample_images\", help=\"images folder for extract_embeddings\")\n",
    "    p.add_argument(\"--out\", default=\"embeddings.csv\", help=\"output csv for embeddings\")\n",
    "    p.add_argument(\"--host\", default=\"127.0.0.1\")\n",
    "    p.add_argument(\"--port\", default=8000, type=int)\n",
    "    p.add_argument(\"--image\", help=\"image path for test_request\")\n",
    "    # convenience: allow passing basic soil values for test_request (optional)\n",
    "    p.add_argument(\"--PH\", type=float, default=6.8)\n",
    "    p.add_argument(\"--EC_ds_m\", type=float, default=0.35)\n",
    "    p.add_argument(\"--OC_pct\", type=float, default=1.0)\n",
    "    p.add_argument(\"--N_kg_hectre\", type=float, default=100.0)\n",
    "    p.add_argument(\"--P_kg_hectre\", type=float, default=20.0)\n",
    "    p.add_argument(\"--K_kg_hectre\", type=float, default=120.0)\n",
    "    p.add_argument(\"--S_ppm\", type=float, default=10.0)\n",
    "    p.add_argument(\"--Zn_ppm\", type=float, default=2.0)\n",
    "    p.add_argument(\"--B_ppm\", type=float, default=0.5)\n",
    "    p.add_argument(\"--Fe_ppm\", type=float, default=30.0)\n",
    "    p.add_argument(\"--Mn_ppm\", type=float, default=5.0)\n",
    "    p.add_argument(\"--Cu_ppm\", type=float, default=1.0)\n",
    "\n",
    "    args = p.parse_args()\n",
    "\n",
    "    if args.mode == \"audit\":\n",
    "        audit_models()\n",
    "    elif args.mode == \"extract_embeddings\":\n",
    "        if not Path(args.images_dir).exists():\n",
    "            print(\"images_dir does not exist:\", args.images_dir)\n",
    "            return\n",
    "        extract_embeddings(args.images_dir, args.out)\n",
    "    elif args.mode == \"runserver\":\n",
    "        app = create_app()\n",
    "        print(f\"Starting server on {args.host}:{args.port} ...\")\n",
    "        uvicorn.run(app, host=args.host, port=args.port)\n",
    "    elif args.mode == \"test_request\":\n",
    "        if not args.image:\n",
    "            print(\"Provide --image for test_request\")\n",
    "            return\n",
    "        # assemble soil dict using cleaned keys\n",
    "        soil = {\n",
    "            \"PH\": args.PH,\n",
    "            \"EC_ds_m\": args.EC_ds_m,\n",
    "            \"OC_pct\": args.OC_pct,\n",
    "            \"N_kg_hectre\": args.N_kg_hectre,\n",
    "            \"P_kg_hectre\": args.P_kg_hectre,\n",
    "            \"K_kg_hectre\": args.K_kg_hectre,\n",
    "            \"S_ppm\": args.S_ppm,\n",
    "            \"Zn_ppm\": args.Zn_ppm,\n",
    "            \"B_ppm\": args.B_ppm,\n",
    "            \"Fe_ppm\": args.Fe_ppm,\n",
    "            \"Mn_ppm\": args.Mn_ppm,\n",
    "            \"Cu_ppm\": args.Cu_ppm\n",
    "        }\n",
    "        server_url = f\"http://{args.host}:{args.port}\"\n",
    "        send_test_request(server_url, args.image, soil)\n",
    "    else:\n",
    "        print(\"Unknown mode\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e174ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl.exe -X POST -F \"PH=6.8\" -F \"EC_ds_m=0.35\" -F \"OC_pct=1.2\" -F \"N_kg_hectre=100\" -F \"P_kg_hectre=20\" -F \"K_kg_hectre=120\" -F \"S_ppm=10\" -F \"Zn_ppm=2\" -F \"B_ppm=0.5\" -F \"Fe_ppm=30\" -F \"Mn_ppm=5\" -F \"Cu_ppm=1\" \"http://127.0.0.1:8000/predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    " curl.exe -X POST -F \"image=@sample_images/1.21.jpeg\" http://127.0.0.1:8000/predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
